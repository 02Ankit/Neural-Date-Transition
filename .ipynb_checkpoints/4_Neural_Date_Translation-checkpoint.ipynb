{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PwQV66FsfAyi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/ujjawald/miniconda3/envs/ujju/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import random\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CU1ZQ8zLa-B3",
    "outputId": "4e56c020-e393-402c-8105-24f0bd939b09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NyIZjnXuOwO4"
   },
   "outputs": [],
   "source": [
    "# Stop words, as we don't need any day name for prediction\n",
    "daynames = {'mon','monday','tue','tuesday','wed','wednesday','thu','thursday','fri','friday','sat','saturday','sun','sunday'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month = {'jan': 'm1', 'january': 'm1',\n",
    "#          'feb': 'm2', 'february': 'm2',\n",
    "#          'mar': 'm3', 'march': 'm3',\n",
    "#          'apr': 'm4', 'april': 'm4',\n",
    "#          'may': 'm5', 'may': 'm5',\n",
    "#          'jun': 'm6', 'june': 'm6',\n",
    "#          'jul': 'm7', 'july': 'm7',\n",
    "#          'aug': 'm8', 'august': 'm8',\n",
    "#          'sep': 'm9', 'september': 'm9',\n",
    "#          'oct': 'm10', 'october': 'm10',\n",
    "#          'nov': 'm11', 'november': 'm11',\n",
    "#          'dec': 'm12', 'december': 'm12',\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9ofC3Ch1lQi6"
   },
   "outputs": [],
   "source": [
    "train_path = open(\"./Data/Ass4/Assignment4aDataset.txt\",'r')\n",
    "train_list = train_path.read().split(\"\\n\")\n",
    "train_list = train_list[:len(train_list)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3djzC86RlddG",
    "outputId": "7293e5d6-7aa9-4f20-d5e2-67ef0fbdbb2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "# Separating X's and y's; human_date = X && machine_date = y\n",
    "human_date = []\n",
    "machine_date = []\n",
    "for i in train_list:\n",
    "  temp_Xy = i.split(\", \")\n",
    "  # if(len(temp_Xy) > 2):              #For checking if there are commas(,) in input/output text\n",
    "  #   print(i)\n",
    "  human_date.append(temp_Xy[0])\n",
    "  machine_date.append(temp_Xy[1])\n",
    "\n",
    "print(len(human_date))\n",
    "print(len(machine_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6yPcq64BljHh",
    "outputId": "7ccfc5fa-0e2e-46a1-83c9-0b42b4b7eda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'wed 1776 11 september'\n",
      "'1776-09-11'\n",
      "(40000,)\n",
      "(40000,)\n",
      "wed 1776 11 september\n",
      "<1776-09-11>\n"
     ]
    }
   ],
   "source": [
    "print(human_date[6])\n",
    "print(machine_date[6])\n",
    "\n",
    "human_date = np.char.lower(np.char.replace(human_date,\"'\",\"\"))\n",
    "\n",
    "# Adding Start(<) and End(>)\n",
    "#IMPORTANCE of Start(<) and End(>)\n",
    "# If Start was not there; then during testing, what will be the input to decoder at t=0\n",
    "# If End was not there; then not able to correctly predict last char\n",
    "                                                                    # 1830-10-22 -> 1830-10-2-\n",
    "                                                                    # 1893-03-24 -> 1893-03-2-\n",
    "                                                                    # 1856-04-12 -> 1856-04-1-\n",
    "            \n",
    "SOS = '<'\n",
    "EOS = '>'\n",
    "\n",
    "for i in range(len(machine_date)):\n",
    "  machine_date[i] = \"<\" + machine_date[i] + \">\"\n",
    "\n",
    "machine_date = np.char.lower(np.char.replace(machine_date,\"'\",\"\"))\n",
    "\n",
    "print(human_date.shape)\n",
    "print(machine_date.shape)\n",
    "\n",
    "print(human_date[6])\n",
    "print(machine_date[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UQFxCwtny9ut"
   },
   "outputs": [],
   "source": [
    "def find_vocab(corpus):\n",
    "  vocab = set()\n",
    "  for document in corpus:\n",
    "    vocab = vocab.union(set(document))\n",
    "  return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eThbc-Nh0HX5",
    "outputId": "863bdcd3-f01d-4647-acaf-89d180405f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n",
      "['-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<', '>']\n"
     ]
    }
   ],
   "source": [
    "input_vocab = list(find_vocab(human_date))\n",
    "output_vocab = list(find_vocab(machine_date))\n",
    "\n",
    "input_vocab.sort()\n",
    "output_vocab.sort()\n",
    "\n",
    "input_vocab_size = len(input_vocab)\n",
    "output_vocab_size = len(output_vocab)\n",
    "\n",
    "print(input_vocab)\n",
    "print(output_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTv4LYATgXcs"
   },
   "source": [
    "Fuctions for PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vy78fyrsiys",
    "outputId": "2c1b5086-59bc-4afb-88d6-40817c28e687"
   },
   "outputs": [],
   "source": [
    "# Function for OneHotEncoding\n",
    "def to_hardmax_vec(num, max_num):\n",
    "  res = np.zeros(max_num)\n",
    "  res[num] = 1\n",
    "  return res\n",
    "\n",
    "# Creating dictonaries for input_vocab\n",
    "input_index_to_char = dict()\n",
    "input_hardmax_dict = dict()\n",
    "for i, item in enumerate(input_vocab):\n",
    "  input_index_to_char[i] = item\n",
    "  input_hardmax_dict[item] = to_hardmax_vec(i,input_vocab_size)\n",
    "\n",
    "# Creating dictonaries for output_vocab\n",
    "output_index_to_char = dict()\n",
    "output_hardmax_dict = dict()\n",
    "for i, item in enumerate(output_vocab):\n",
    "  output_index_to_char[i] = item\n",
    "  output_hardmax_dict[item] = to_hardmax_vec(i,output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "564S5yht7QaI"
   },
   "outputs": [],
   "source": [
    "def human_date_to_char_list(sample):\n",
    "  #Tokeninzing\n",
    "  tokens = sample.split(' ')\n",
    "  filtered_tokens = []\n",
    "  for i in tokens:\n",
    "    if i not in daynames:\n",
    "      filtered_tokens.append(i)\n",
    "#   print(filtered_tokens)\n",
    "  # ->Filtered Token; len == 1(['21/03/2046']) or len == 3(['22', 'december', '1600'])\n",
    "\n",
    "  # Converting filtered token to list of char\n",
    "  char_list = []\n",
    "  n = len(filtered_tokens)\n",
    "  for i in range(n):\n",
    "    curr_char_list = list(filtered_tokens[i])\n",
    "    if(i != n-1):\n",
    "      curr_char_list.append(' ')\n",
    "    char_list.extend(curr_char_list)\n",
    "  return char_list\n",
    "\n",
    "\n",
    "def char_list_to_vector(char_list, max_len):\n",
    "  # Padding\n",
    "  curr_len = len(char_list)\n",
    "  for i in range(max_len-curr_len):\n",
    "    char_list.append(' ')\n",
    "  \n",
    "  # Replacing chars by OneHotEmbeddings  \n",
    "  vector_date_rep = []\n",
    "  for i in char_list:\n",
    "    vector_date_rep.append(input_hardmax_dict[i])\n",
    "  \n",
    "  return vector_date_rep\n",
    "\n",
    "def machine_date_to_hardmax(sample):\n",
    "  char_list = list(sample)\n",
    "  vector_date_rep = []\n",
    "  for i in char_list:\n",
    "    vector_date_rep.append(output_hardmax_dict[i]);\n",
    "  return vector_date_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rIJSJe6UbW1J",
    "outputId": "bc02ef19-7b87-4a90-8073-e17833851f15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 17, 34)\n",
      "(40000, 12, 13)\n"
     ]
    }
   ],
   "source": [
    "complete_char_list = [human_date_to_char_list(sample) for sample in human_date]\n",
    "# For Padding\n",
    "max_len = 0\n",
    "for char_list in complete_char_list:\n",
    "  max_len = max(max_len, len(char_list))\n",
    "\n",
    "train_X = [char_list_to_vector(char_list,max_len) for char_list in complete_char_list]\n",
    "train_X = np.array(train_X)\n",
    "print(train_X.shape)\n",
    "\n",
    "train_y = [machine_date_to_hardmax(sample) for sample in machine_date]\n",
    "train_y = np.array(train_y)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9tTxxKtv_acN"
   },
   "outputs": [],
   "source": [
    "n_samples = train_X.shape[0]\n",
    "# HyperParameters\n",
    "batch_size = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EokUEEgkk0Bs"
   },
   "source": [
    "Creating DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "udnWdDx8kzPW"
   },
   "outputs": [],
   "source": [
    "train_X = torch.Tensor(train_X)\n",
    "train_y = torch.Tensor(train_y)\n",
    "\n",
    "# train_y = train_y.type(torch.LongTensor)\n",
    "train_tensor_Xy = TensorDataset(train_X,train_y)\n",
    "\n",
    "train_size = int(n_samples*(0.8))\n",
    "val_size = n_samples - train_size\n",
    "train_data, valid_data = random_split(train_tensor_Xy, [train_size,val_size])\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_data_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/Ass4/Assignment4aTestDataset.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Data/Ass4/Assignment4aTestDataset.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_list \u001b[38;5;241m=\u001b[39m test_path\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m test_list \u001b[38;5;241m=\u001b[39m test_list[:\u001b[38;5;28mlen\u001b[39m(test_list)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/ujju/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/Ass4/Assignment4aTestDataset.txt'"
     ]
    }
   ],
   "source": [
    "test_path = open(\"./Data/Ass4/Assignment4aTestDataset.txt\",'r')\n",
    "test_list = test_path.read().split(\"\\n\")\n",
    "test_list = test_list[:len(test_list)-1]\n",
    "\n",
    "human_date_test = []\n",
    "machine_date_test = []\n",
    "for i in test_list:\n",
    "  temp_Xy = i.split(\", \")\n",
    "  # if(len(temp_Xy) > 2):              #For checking if there are commas(,) in input/output text\n",
    "  #   print(i)\n",
    "  human_date_test.append(temp_Xy[0])\n",
    "  machine_date_test.append(temp_Xy[1])\n",
    "\n",
    "print(len(human_date_test))\n",
    "print(len(machine_date_test))\n",
    "\n",
    "\n",
    "human_date_test = np.char.lower(np.char.replace(human_date_test,\"'\",\"\"))\n",
    "for i in range(len(machine_date_test)):\n",
    "  machine_date_test[i] = \"<\" + machine_date_test[i] + \">\"\n",
    "machine_date_test = np.char.lower(np.char.replace(machine_date_test,\"'\",\"\"))\n",
    "\n",
    "complete_char_list_test = [human_date_to_char_list(sample) for sample in human_date_test]\n",
    "max_len = 0\n",
    "for char_list in complete_char_list:\n",
    "  max_len = max(max_len, len(char_list))\n",
    "test_X = [char_list_to_vector(char_list,max_len) for char_list in complete_char_list_test]\n",
    "test_X = np.array(test_X)\n",
    "print(test_X.shape)\n",
    "\n",
    "test_y = [machine_date_to_hardmax(sample) for sample in machine_date_test]\n",
    "test_y = np.array(test_y)\n",
    "print(test_y.shape)\n",
    "\n",
    "test_X = torch.Tensor(test_X)\n",
    "test_y = torch.Tensor(test_y)\n",
    "\n",
    "test_data = TensorDataset(test_X,test_y)\n",
    "\n",
    "test_data_loader = DataLoader(test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMOh3yr9ypaF"
   },
   "source": [
    "Class for **Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9thpR56vpGu"
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "  def __init__(self,input_size, hidden_size, num_layers=1):\n",
    "    super(encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    # define LSTM layer\n",
    "    self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True)\n",
    "\n",
    "  def forward(self, x_input):\n",
    "    lstm_out, self.hidden = self.lstm(x_input)\n",
    "    return lstm_out, self.hidden   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TCSByq36JG6"
   },
   "source": [
    "Class for **Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIQ6XK523gU1"
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "  def __init__(self,input_size, hidden_size, output_size, input_len = None, num_layers=1):\n",
    "    super(decoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    # define LSTM layer\n",
    "    self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True)\n",
    "    self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x_input, hidden_state, encoder_hidden_states = None):\n",
    "    x_input = x_input.float().unsqueeze(1)\n",
    "    lstm_out, self.hidden = self.lstm(x_input,hidden_state)\n",
    "    lstm_out = lstm_out.squeeze(1)\n",
    "    output_recursive = self.linear(lstm_out)\n",
    "    return output_recursive, self.hidden, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for **Attention Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_decoder(nn.Module):\n",
    "  def __init__(self,input_size, hidden_size, output_size, input_len, num_layers=1):\n",
    "    super(attention_decoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    # define LSTM layer\n",
    "    self.proj_input = nn.Linear(output_size, hidden_size)         #For converting input to size of hidden layer\n",
    "    self.attention_linear1 = nn.Linear(2*hidden_size, input_len)  # Attention weights, concatenated size to input len size\n",
    "    self.attention_linear2 = nn.Linear(2*hidden_size, output_size)# From 2*hidden size to output_size as we need to give it as input to decoder\n",
    "    self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True)\n",
    "    self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x_input, hidden_state, encoder_hidden_states):\n",
    "    x_input = x_input.float().unsqueeze(1)\n",
    "    projected_input = self.proj_input(x_input)\n",
    "    attention_weights = self.attention_linear1(torch.cat( (projected_input,(hidden_state[0].squeeze(0)).unsqueeze(1)), axis = 2))\n",
    "    attention_weights = F.softmax(attention_weights, dim = 2)\n",
    "    \n",
    "    weights = torch.bmm(attention_weights, encoder_hidden_states) #Batch Matrix Multiplication\n",
    "    output = torch.cat((projected_input, weights), 2)\n",
    "    output = self.attention_linear2(output)\n",
    "    output = F.relu(output)\n",
    "    \n",
    "    lstm_out, self.hidden = self.lstm(output,hidden_state)\n",
    "    lstm_out = lstm_out.squeeze(1)\n",
    "    output_recursive = self.linear(lstm_out)\n",
    "    return output_recursive, self.hidden, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVjVuLRQ6Qf3"
   },
   "source": [
    "Class for **Encoder-Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EM4wKFrSfrg"
   },
   "outputs": [],
   "source": [
    "class seq_to_seq(nn.Module):\n",
    "  \n",
    "  def __init__(self, encoder_input_size, decoder_input_size,  hidden_size, decoder_output_size, input_len, is_attention = True):\n",
    "    super(seq_to_seq, self).__init__()\n",
    "\n",
    "    self.encoder_input_size = encoder_input_size\n",
    "    self.decoder_input_size = decoder_input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.input_len = input_len\n",
    "    self.is_attention = is_attention\n",
    "#     self.is_heirarichal = is_heirarichal\n",
    "    self.encoder = encoder(input_size = encoder_input_size, hidden_size = hidden_size).to(device)\n",
    "    if(is_attention == True):\n",
    "      self.decoder = attention_decoder(input_size = decoder_input_size, hidden_size = hidden_size, output_size = decoder_output_size, input_len = input_len).to(device)\n",
    "    else:\n",
    "      self.decoder = decoder(input_size = decoder_input_size, hidden_size = hidden_size, output_size = decoder_output_size).to(device)\n",
    "\n",
    "    # List for storning and plotting graph of accuracy and loss, per epoch\n",
    "    self.train_loss_list_plotting = []\n",
    "    self.valid_loss_list_plotting = []\n",
    "\n",
    "    self.train_accuracy_plotting = []\n",
    "    self.valid_accuracy_plotting = []\n",
    "\n",
    "    self.train_charwise_accuracy_plotting = []\n",
    "    self.valid_charwise_accuracy_plotting = []\n",
    "    \n",
    "    \n",
    "  def find_accuracy_batch(self,true_y,pred_y):\n",
    "    #Not counting start and end symbol in accuracy\n",
    "    true_y = true_y[:,1:-1,:]\n",
    "    pred_y = pred_y[:,1:-1,:]\n",
    "    \n",
    "    true_y = torch.argmax(true_y, dim=2)\n",
    "    pred_y = torch.argmax(pred_y, dim=2)\n",
    "    compared_ys = (true_y == pred_y)\n",
    "    compared_ys = torch.sum(compared_ys, axis = 1, keepdims = True)\n",
    "    real_ys = torch.full((batch_size,1), 10).to(device)\n",
    "    return torch.sum(compared_ys == real_ys)/compared_ys.shape[0]\n",
    "\n",
    "\n",
    "  def find_charwise_accuracy_batch(self,true_y,pred_y):\n",
    "    #Not counting start and end symbol in accuracy\n",
    "    true_y = true_y[:,1:-1,:]\n",
    "    pred_y = pred_y[:,1:-1,:]\n",
    "    \n",
    "    true_y = torch.argmax(true_y, dim=2)\n",
    "    pred_y = torch.argmax(pred_y, dim=2)\n",
    "    compared_ys = (true_y == pred_y)\n",
    "    return torch.sum(compared_ys)/(compared_ys.shape[0] * compared_ys.shape[1])\n",
    "\n",
    "\n",
    "  def find_charwise_separate_accuracy__batch(self,true_y,pred_y):\n",
    "    #Not counting start and end symbol in accuracy\n",
    "    true_y = true_y[:,1:-1,:]\n",
    "    pred_y = pred_y[:,1:-1,:]\n",
    "    true_y = torch.argmax(true_y, dim=2)\n",
    "    pred_y = torch.argmax(pred_y, dim=2)\n",
    "    compared_ys = (true_y == pred_y)\n",
    "    return (torch.sum(compared_ys, axis = 0)/(compared_ys.shape[0])).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "  def print_date(self, true_y, pred_y):\n",
    "    true_y = torch.argmax(true_y, dim=2)\n",
    "    pred_y = torch.argmax(pred_y, dim=2)\n",
    "    \n",
    "    # Bringing it back to CPU\n",
    "    true_y = true_y.detach().cpu().numpy()\n",
    "    pred_y = pred_y.detach().cpu().numpy()\n",
    "    for i in range(true_y.shape[0]):\n",
    "      true_date = \"\"\n",
    "      pred_date = \"\"\n",
    "      for j in range(1,true_y.shape[1]-1):\n",
    "        true_date += output_index_to_char[true_y[i][j]]\n",
    "        pred_date += output_index_to_char[pred_y[i][j]]\n",
    "      print(true_date , \"->\", pred_date)\n",
    "\n",
    "\n",
    "  def forward_encoder_decoder(self, x , y = None , target_len = 12, teacher_forcing_ratio = 0):\n",
    "    x = x.to(device)\n",
    "    if(not y == None):\n",
    "      y = y.to(device)\n",
    "    batch_size = x.shape[0]\n",
    "    # ENCODER SIDE\n",
    "    # outputs tensor for calculating loss\n",
    "    decoder_output_tensor = torch.zeros(batch_size, target_len, self.decoder.output_size).to(device)\n",
    "    #Attention weights for visualizing\n",
    "    attention_weights_tensor = None\n",
    "    if(self.is_attention == True):\n",
    "      attention_weights_tensor = torch.zeros(batch_size, target_len, self.input_len).to(device)\n",
    "    # Encoder Outputs\n",
    "    encoder_output, encoder_hidden_state = self.encoder(x)\n",
    "\n",
    "    # DECODER SIDE\n",
    "    # Initializing Input & Hidden State for Decoder\n",
    "    if (y == None):\n",
    "      decoder_input = torch.FloatTensor([output_hardmax_dict[SOS] for i in range(batch_size)]).to(device)\n",
    "    else:\n",
    "      decoder_input = y[:,0,:]\n",
    "    decoder_hidden = encoder_hidden_state\n",
    "\n",
    "    # First input to decoder is true character at time-step 0\n",
    "    for t in range(1, target_len):\n",
    "\n",
    "      decoder_output, decoder_hidden, attention_weights = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "      decoder_output_tensor[:,t-1,:] = decoder_output\n",
    "      if(self.is_attention == True):\n",
    "        attention_weights_tensor[:,t-1,:] = attention_weights.squeeze(1)\n",
    "      # predict with teacher forcing\n",
    "      if random.random() < teacher_forcing_ratio:\n",
    "        decoder_input = y[:, t, :]\n",
    "      # predict recursively \n",
    "      else:\n",
    "        decoder_input = decoder_output\n",
    "\n",
    "    return decoder_output_tensor, y, attention_weights_tensor\n",
    "\n",
    "    \n",
    "  def fit(self, train_data_loader, valid_data_loader, n_epochs, target_len, batch_size, teacher_forcing_ratio = 0.5, learning_rate = 0.01):\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "      train_loss_list_batch = []\n",
    "      valid_loss_list_batch = []\n",
    "      train_accuracy_list_batch = []\n",
    "      valid_accuracy_list_batch = []\n",
    "      train_charwise_accuracy_list_batch = []\n",
    "      valid_charwise_accuracy_list_batch = []\n",
    "      \n",
    "      for batch in train_data_loader:\n",
    "        x, y = batch\n",
    "        # Forward Function for complete architecture\n",
    "        decoder_output, true_y, attention_weights_tensor = self.forward_encoder_decoder(x, y, target_len, teacher_forcing_ratio)\n",
    "        true_y = true_y.float()\n",
    "        \n",
    "        # zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the loss\n",
    "        # Not counting start and end symbol in loss\n",
    "        loss = loss_function(decoder_output[:,1:-1,:], true_y[:,1:-1,:])\n",
    "        curr_batch_train_loss = loss.item()\n",
    "\n",
    "        # Calculating Loss and Accuracy and appending in List\n",
    "        train_loss_list_batch.append(curr_batch_train_loss)\n",
    "        train_accuracy_list_batch.append(self.find_accuracy_batch(true_y,decoder_output))\n",
    "        train_charwise_accuracy_list_batch.append(self.find_charwise_accuracy_batch(true_y,decoder_output))\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "      for batch in valid_data_loader:\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "          decoder_output, true_y, attention_weights_tensor = self.forward_encoder_decoder(x, y)\n",
    "          true_y = true_y.float()\n",
    "#           self.print_date(true_y, decoder_output)\n",
    "          loss = loss_function(decoder_output[:,1:-1,:], true_y[:,1:-1,:])\n",
    "          curr_batch_valid_loss = loss.item()\n",
    "\n",
    "        valid_loss_list_batch.append(curr_batch_valid_loss)\n",
    "        valid_accuracy_list_batch.append(self.find_accuracy_batch(true_y,decoder_output))\n",
    "        valid_charwise_accuracy_list_batch.append(self.find_charwise_accuracy_batch(true_y,decoder_output))\n",
    "\n",
    "      # loss for epoch \n",
    "      train_loss_epoch = torch.tensor(train_loss_list_batch).mean()\n",
    "      valid_loss_epoch = torch.tensor(valid_loss_list_batch).mean()\n",
    "\n",
    "      train_accuracy_epoch = torch.tensor(train_accuracy_list_batch).mean()\n",
    "      valid_accuracy_epoch = torch.tensor(valid_accuracy_list_batch).mean()\n",
    "      \n",
    "      train_charwise_accuracy_epoch = torch.tensor(train_charwise_accuracy_list_batch).mean()\n",
    "      valid_charwise_loss_epoch = torch.tensor(valid_charwise_accuracy_list_batch).mean()\n",
    "      \n",
    "      print('\\033[1m', \"Epoch Number = \", i, '\\033[0m')\n",
    "      print(\"Training Loss = \", \"{:.4f}\".format(float(train_loss_epoch)), \"; Validation Loss = \", \"{:.4f}\".format(float(valid_loss_epoch)))\n",
    "      print(\"Train Accuracy = \", \"{:.4f}\".format(float(train_accuracy_epoch)*100), \"%; Validation Accuracy = \", \"{:.4f}\".format(float(valid_accuracy_epoch)*100), \"%\")\n",
    "      print(\"Train Charwise Accuracy = \", \"{:.4f}\".format(float(train_charwise_accuracy_epoch)*100), \"%; \", \"; Validation Charwise Accuracy = \", \"{:.4f}\".format(float(valid_charwise_loss_epoch)*100), \"%\")\n",
    "      \n",
    "      self.train_loss_list_plotting.append(train_loss_epoch)\n",
    "      self.valid_loss_list_plotting.append(valid_loss_epoch)\n",
    "      self.train_accuracy_plotting.append(train_accuracy_epoch * 100)\n",
    "      self.valid_accuracy_plotting.append(valid_accuracy_epoch * 100)\n",
    "      self.train_charwise_accuracy_plotting.append(train_charwise_accuracy_epoch * 100)\n",
    "      self.valid_charwise_accuracy_plotting.append(valid_charwise_loss_epoch * 100)\n",
    "  \n",
    "  def predict(self, test_data_loader):\n",
    "    accuracy_list_pred = []\n",
    "    accuracy_charwise_separate_list = []\n",
    "    accuracy_charwise_list = []\n",
    "    for batch in test_data_loader:\n",
    "      x, y = batch\n",
    "      with torch.no_grad():\n",
    "        decoder_output, true_y, attention_weights_tensor = self.forward_encoder_decoder(x, y)\n",
    "        true_y = true_y.float()\n",
    "      accuracy_charwise_separate_list.append(self.find_charwise_separate_accuracy__batch(true_y,decoder_output))\n",
    "      accuracy_list_pred.append(self.find_accuracy_batch(true_y,decoder_output))\n",
    "      accuracy_charwise_list.append(self.find_charwise_accuracy_batch(true_y,decoder_output))\n",
    "        \n",
    "    accuracy_charwise_separate_list = list(np.mean(np.array(accuracy_charwise_separate_list), axis = 0))\n",
    "    accuracy = torch.tensor(accuracy_list_pred).mean() * 100\n",
    "    accuracy_charwise = torch.tensor(accuracy_charwise_list).mean() * 100\n",
    "    print('\\033[1m' + \"Accuracy on Test Data = \", \"{:.4f}\".format(accuracy), \" %\" + '\\033[0m' )\n",
    "    print('\\033[1m' + \"Charwise Accuracy on Test Data = \", \"{:.4f}\".format(accuracy_charwise), \" %\" + '\\033[0m' )\n",
    "    print(\"Character-wise Accuracy:\")\n",
    "    print(accuracy_charwise_separate_list)\n",
    "    \n",
    "    \n",
    "  # Function for visualizing attention\n",
    "  def visualize(self, given_date):\n",
    "    # Converting given date string to vectors\n",
    "    complete_char_list = human_date_to_char_list(given_date)\n",
    "    x = [char_list_to_vector(complete_char_list,self.input_len)]\n",
    "    x = torch.Tensor(np.array(x))\n",
    "    x = x.to(device)\n",
    "    print(x.shape)\n",
    "    \n",
    "    # Predicting the output date\n",
    "    with torch.no_grad():\n",
    "      decoder_output, true_y, attention_weights_tensor = self.forward_encoder_decoder(x)\n",
    "    print(decoder_output.shape)\n",
    "    pred_y = torch.argmax(decoder_output,axis = 2).detach().cpu().numpy().squeeze(0)\n",
    "    \n",
    "    print(pred_y.shape)\n",
    "    # Converting obtained vector_y to string_y\n",
    "    pred_date = \"\"\n",
    "    for j in range(1,pred_y.shape[0]-1):\n",
    "      pred_date += output_index_to_char[pred_y[j]]\n",
    "    \n",
    "    true_date = given_date\n",
    "    print(true_date , \"->\", pred_date)\n",
    "    \n",
    "    # Visualizing\n",
    "    plt.imshow(attention_weights_tensor.detach().cpu().numpy().squeeze(0))\n",
    "    x_pos = np.arange(len(true_date))\n",
    "    y_pos = np.arange(len(pred_date))\n",
    "    plt.xticks(x_pos, list(true_date), color='black', fontsize='14')\n",
    "    plt.yticks(y_pos, list(pred_date), color='black', fontsize='14', horizontalalignment='right')\n",
    "    plt.show()\n",
    "    \n",
    "  # Function to check which input dates are predicted wrong\n",
    "  def check_wrong_dates(self, val_data_loader_bs1):\n",
    "    for batch in val_data_loader_bs1:\n",
    "      x, y = batch\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "      with torch.no_grad():\n",
    "        pred_y, true_y, attention_weights_tensor = self.forward_encoder_decoder(x, y)\n",
    "      true_y = true_y.float()\n",
    "      true_y = torch.argmax(true_y, dim=2)\n",
    "      pred_y = torch.argmax(pred_y, dim=2)\n",
    "    \n",
    "      # Bringing it back to CPU\n",
    "      true_y = true_y.detach().cpu().numpy()\n",
    "      pred_y = pred_y.detach().cpu().numpy()\n",
    "      for i in range(true_y.shape[0]):\n",
    "        true_date = \"\"\n",
    "        pred_date = \"\"\n",
    "        for j in range(1,true_y.shape[1]-1):\n",
    "          true_date += output_index_to_char[true_y[i][j]]\n",
    "          pred_date += output_index_to_char[pred_y[i][j]]\n",
    "        if(true_date != pred_date):\n",
    "          x = torch.argmax(x, dim=2)\n",
    "          x = (x.squeeze(0)).detach().cpu().numpy()\n",
    "          date = \"\"\n",
    "          for i in range(x.shape[0]):\n",
    "            date += input_index_to_char[x[i]]\n",
    "          print(date, \"->\", true_date , \"->\", pred_date)\n",
    "    \n",
    "  def plot_loss_curve(self, flag):\n",
    "    x = [(i+1) for i in range(len(self.train_loss_list_plotting))]\n",
    "    plt.xlabel('#Epoch')\n",
    "    if(flag < 2):\n",
    "        plt.ylabel('Loss')\n",
    "    else:\n",
    "        plt.ylabel('Accuracy')\n",
    "    temp_str = \"\"\n",
    "    if(flag == 0):\n",
    "        temp_str += \"Loss curve for Train Data\"\n",
    "        plt.plot(x,self.train_loss_list_plotting)\n",
    "    elif(flag == 1):\n",
    "        temp_str += \"Loss curve for Validation Data\"\n",
    "        plt.plot(x,self.valid_loss_list_plotting)\n",
    "    elif(flag == 2):\n",
    "        temp_str += \"Accuracy curve for Train Data\"\n",
    "        plt.plot(x,self.train_accuracy_plotting)\n",
    "    elif(flag == 3):\n",
    "        temp_str += \"Accuracy curve for Validation Data\"\n",
    "        plt.plot(x,self.valid_accuracy_plotting)\n",
    "    elif(flag == 4):\n",
    "        temp_str += \"Average Character-Wise Accuracy curve for Train Data\"\n",
    "        plt.plot(x,self.train_charwise_accuracy_plotting)\n",
    "    else:\n",
    "        temp_str += \"Average Character-Wise Accuracy curve for Validation Data\"\n",
    "        plt.plot(x,self.valid_charwise_accuracy_plotting)\n",
    "    plt.title(temp_str)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "c0xc_sFqkWrx",
    "outputId": "3838fca0-7904-4cf7-a968-b19f2ea86f6b"
   },
   "outputs": [],
   "source": [
    "# specify model parameters and train\n",
    "n_epochs =  25\n",
    "lr = 0.005\n",
    "model_normal = seq_to_seq(encoder_input_size = train_X.shape[2], decoder_input_size = train_y.shape[2], hidden_size = 200, decoder_output_size = output_vocab_size, input_len = train_X.shape[1], is_attention = False).to(device)\n",
    "model_normal.fit(train_data_loader, valid_data_loader, n_epochs = n_epochs, target_len = train_y.shape[1], batch_size = batch_size, teacher_forcing_ratio = 0.5, learning_rate = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "for i in range(6):\n",
    "    model_normal.plot_loss_curve(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_normal.predict(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking wrongly predicted dates in validation dataset\n",
    "# val_data_loader_bs1 = DataLoader(valid_data, batch_size = 1)\n",
    "# model_normal.check_wrong_dates(test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model parameters and train\n",
    "n_epochs =  30\n",
    "lr = 0.005\n",
    "model_attention = seq_to_seq(encoder_input_size = train_X.shape[2], decoder_input_size = train_y.shape[2], hidden_size = 200, decoder_output_size = output_vocab_size, input_len = train_X.shape[1], is_attention = True).to(device)\n",
    "model_attention.fit(train_data_loader, valid_data_loader, n_epochs = n_epochs, target_len = train_y.shape[1], batch_size = batch_size, teacher_forcing_ratio = 0.5, learning_rate = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
    "for i in range(6):\n",
    "    model_normal.plot_loss_curve(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attention.predict(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking wrongly predicted dates in validation dataset\n",
    "# val_data_loader_bs1 = DataLoader(valid_data, batch_size = 1)\n",
    "# model_attention.check_wrong_dates(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,12)\n",
    "model_attention.visualize(\"19 feb 2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MSELoss works but CrossEntropy doesn't, reason?? Logic says that CE should also work as it is a classification task.\n",
    "2. Adam works but SGD doesn't. This is because, SGD is not able to move through the plateau region while Adam moves quicly throgh plateau.\n",
    "3. SOS and EOS are important!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
